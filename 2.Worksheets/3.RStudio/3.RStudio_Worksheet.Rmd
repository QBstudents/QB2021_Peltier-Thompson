---
title: '3\. Worksheet: Basic R'
author: "Danny Peltier; Z620: Quantitative Biodiversity, Indiana University"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: pdf_document
geometry: margin=2.54cm
---

## OVERVIEW

This worksheet introduces some of the basic features of the R computing environment (http://www.r-project.org).
It is designed to be used along side the **3. RStudio** handout in your binder. 
You will not be able to complete the exercises without the corresponding handout.

## Directions:
1. In the Markdown version of this document in your cloned repo, change "Student Name" on line 3 (above) with your name.
2. Complete as much of the worksheet as possible during class.
3. Use the handout as a guide; it contains a more complete description of data sets along with examples of proper scripting needed to carry out the exercises.
4. Answer questions in the  worksheet.
Space for your answers is provided in this document and is indicated by the ">" character.
If you need a second paragraph be sure to start the first line with ">".
You should notice that the answer is highlighted in green by RStudio (color may vary if you changed the editor theme). 
5. Before you leave the classroom today, it is *imperative* that you **push** this file to your GitHub repo, at whatever stage you are. Ths will enable you to pull your work onto your own computer.
6. When you have completed the worksheet, **Knit** the text and code into a single PDF file by pressing the `Knit` button in the RStudio scripting panel.
This will save the PDF output in your '3.RStudio' folder.
7. After Knitting, please submit the worksheet by making a **push** to your GitHub repo and then create a **pull request** via GitHub.
Your pull request should include this file (**3.RStudio_Worksheet.Rmd**) with all code blocks filled out and questions answered) and the PDF output of `Knitr` (**3.RStudio_Worksheet.pdf**).

The completed exercise is due on **Wednesday, March 24^th^, 2021 before 12:00 PM (noon)**.

## 1) HOW WE WILL BE USING R AND OTHER TOOLS

You are working in an RMarkdown (.Rmd) file.
This allows you to integrate text and R code into a single document.
There are two major features to this document: 1) Markdown formatted text and 2) "chunks" of R code.
Anything in an R code chunk will be interpreted by R when you *Knit* the document.

When you are done, you will *knit* your document together.
However, if there are errors in the R code contained in your Markdown document, you will not be able to knit a PDF file. 
If this happens, you will need to review your code, locate the source of the error(s), and make the appropriate changes.
Even if you are able to knit without issue, you should review the knitted document for correctness and completeness before you submit the Worksheet. Next to the `Knit` button in the RStudio scripting panel there is a spell checker button (`ABC`) button.

## 2) SETTING YOUR WORKING DIRECTORY

In the R code chunk below, please provide the code to: 
1) clear your R environment,
2) print your current working directory, and
3) set your working directory to your '3.RStudio' folder. 

```{r}
rm(list = ls()) #clears environment
getwd() #current wd
setwd("C:/Users/Danny/Desktop/GitHub/QB2021_Peltier-Thompson/2.Worksheets/3.RStudio") #set new wd
getwd() #double check it worked
```

## 3) USING R AS A CALCULATOR

To follow up on the pre-class exercises, please calculate the following in the R code chunk below. 
Feel free to reference the **1. Introduction to version control and computing tools** handout. 

1) the volume of a cube with length, l, = 5 (volume = l^3 )
2) the area of a circle with radius, r, = 2 (area = pi * r^2). 
3) the length of the opposite side of a right-triangle given that the angle, theta, = pi/4. (radians, a.k.a. 45Â°) and with hypotenuse length sqrt(2) (remember: sin(theta) = opposite/hypotenuse).
4) the log (base e) of your favorite number.

```{r}
l <- 5
l^3 #1

r <- 2
pi * r^2 #2

theta <- pi / 4
hypot <- sqrt(2)
sintheta <- sin(theta)
sintheta / hypot #3

favlog <- log(23) #4 

```

## 4) WORKING WITH VECTORS

To follow up on the pre-class exercises, please perform the requested operations in the R-code chunks below.

### Basic Features Of Vectors

In the R-code chunk below, do the following: 
1) Create a vector `x` consisting of any five numbers.
2) Create a new vector `w` by multiplying `x` by 14 (i.e., "scalar").
3) Add `x` and `w` and divide by 15.

```{r}
x <- c(3, 8, 11, 23, 27) #1
w <- x * 14 #2
(x + w) / 15 #3

```

Now, do the following: 
1) Create another vector (`k`) that is the same length as `w`.
2) Multiply `k` by `x`.
3) Use the combine function to create one more vector, `d` that consists of any three elements from `w` and any four elements of `k`.

```{r}
k <- c(9, 8, 2, 1, 2) #1
k * x #2
d <- c(w[2:4], k[2:5]) #3
```

### Summary Statistics of Vectors

In the R-code chunk below, calculate the **summary statistics** (i.e., maximum, minimum, sum, mean, median, variance, standard deviation, and standard error of the mean) for the vector (`v`) provided.

```{r}
v <- c(16.4, 16.0, 10.1, 16.8, 20.5, NA, 20.2, 13.1, 24.8, 20.2, 25.0, 20.5, 30.5, 31.4, 27.1)
maximum <- max(na.omit(v))
minimum <- min(na.omit(v))
totalsum <- sum(na.omit(v))
average <- mean(na.omit(v))
middle <- median(na.omit(v))
variance <- var(na.omit(v))
standdev <- sd(na.omit(v))
sem <- function(v) {standdev / sqrt(length(na.omit(v)))}
stander <- sem(v)

```

## 5) WORKING WITH MATRICES

In the R-code chunk below, do the following:
Using a mixture of Approach 1 and 2 from the **3. RStudio** handout, create a matrix with two columns and five rows.
Both columns should consist of random numbers.
Make the mean of the first column equal to 8 with a standard deviation of 2 and the mean of the second column equal to 25 with a standard deviation of 10.

```{r}
col1 <- rnorm(5, mean = 8, sd = 2)
col2 <- rnorm(5, mean = 25, sd = 10)
rm <- cbind(col1,col2)


```

***Question 1***: What does the `rnorm` function do? 
What do the arguments in this function specify? 
Remember to use `help()` or type `?rnorm`.

> Answer 1: for the rnorm function you enter a value of numbers you want, a mean, and a standard deviation. The function returns a vector of random numbers that fit a normal distribution of the variables you entered. So if i put in x <- rnorm(10, mean = 20, sd = 3) I will get a vector of 10 random numbers that have a mean of 20 and a standard deviation of 3


In the R code chunk below, do the following: 
1) Load `matrix.txt` from the **3.RStudio** data folder as matrix `m`.
2) Transpose this matrix.
3) Determine the dimensions of the transposed matrix.

```{r}
m <- read.delim("data/matrix.txt",header = FALSE) #loaded the matrix
tm <- t(m) #tm is the transposed matrix
dim(tm) #dimensions of tm = 5x10

```


***Question 2***: What are the dimensions of the matrix you just transposed?

> Answer 2: 5 rows by 10 columns 


###Indexing a Matrix

In the R code chunk below, do the following:
1) Index matrix `m` by selecting all but the third column.
2) Remove the last row of matrix `m`.

```{r}
im <- m[-3] #all but the third column
m <- read.delim("data/matrix.txt",header = FALSE) #reload original m
lm <- m[-5]

```

## 6) BASIC DATA VISUALIZATION AND STATISTICAL ANALYSIS
### Load Zooplankton Data Set

In the R code chunk below, do the following:
1) Load the zooplankton data set from the **3.RStudio** data folder.
2) Display the structure of this data set.

```{r}
zoop <- read.delim("data/zoops.txt") #load zooplapnkton dataset as zoopa
str(zoop) #structure of zoop

```

### Correlation

In the R-code chunk below, do the following:
1) Create a matrix with the numerical data in the `meso` dataframe.
2) Visualize the pairwise **bi-plots** of the six numerical variables.
3) Conduct a simple **Pearson's correlation** analysis.

```{r}
meso <- read.table("data/zoop_nuts.txt", sep = "\t", header = TRUE)
str(meso)
meso.num <- meso[,3:8]
pairs(meso.num) #bi-plots
cor1 <- cor(meso.num) #pearsons correlation analysis

```


***Question 3***: Describe some of the general features based on the visualization and correlation analysis above?

> Answer 3: zooplankton biomass is most correlated to the total inorganic nutrient concentration, followed in descending order by total nitrogen concentration, total phosphorus concentration, and soluble reactive phosphorus concentration. Zooplankton biomass is not correlated with chlorophyll a concentration. Total inorganic nutrient concentration, total nitrogen concentration, total phosphorus concentration, and soluble reactive phosphorus concentration are all coorelated with eachother which could explain why they are all fairly correlated with plankton biomass. Nothing is correlated to the chlorophyll a concentration suggesting that it doesn't have a major impact on the zooplankton biomass or the factors actually impact the zooplankton biomass. The reason total inorganic nutrient concentration is the most correlated to zooplankton biomass is because it encompasses the other non-zooplankton factors, aside from that, total Nitrogen concentration would be the nutrient zooplankton biomass is most related to. 


In the R code chunk below, do the following:
1) Redo the correlation analysis using the `corr.test()` function in the `psych` package with the following options: method = "pearson", adjust = "BH".
2) Now, redo this correlation analysis using a non-parametric method.
3) Use the print command from the handout to see the results of each correlation analysis.

```{r}
install.packages("psych", repos="https://cran.rstudio.com") 
require("psych")
cor2 <- corr.test(meso.num, method = "pearson", adjust = "BH")
print(cor2, digits = 3, short = FALSE)
cor3 <- corr.test(meso.num, method = "spearman", adjust = "BH")
print(cor3, digits = 3, short = FALSE)

install.packages("corrplot", repos="http://cran.rstudio.com")
require(corrplot)
corrplot(cor1, method = "ellipse")

```

***Question 4***: 
Describe what you learned from `corr.test`. 
Specifically, are the results sensitive to whether you use parametric (i.e., Pearson's) or non-parametric methods?
When should one use non-parametric methods instead of parametric methods?
With the Pearson's method, is there evidence for false discovery rate due to multiple comparisons? 
Why is false discovery rate important?

> Answer 4: corr.test does the correlation analysis you tell it to do but it also shows the p-values to each correlation. I've never taken a statistics class so I'm not 100% sure about my answers to this and would definitly like feedback. Parametric methods analyze the means and assumes that the data follows a normal distribution; however, the data doesnt have to have a normal distribution if its a large data set.Parametric methods can be heavily affected if your data has outliers because it analyzes means. Non-parametric methods analyze the medians and theres no assumption that the data have normal distributions. Non-parametric methods are good to use if you have a small sample size, the data is better represented by the median, or if the data has major outliers. The results are sensitive to the different methods. Both analyses had low correlation values and high P-values for CHLA comparisons, which were drastically different than other variables, but parametric methods had larger ranges and more extreme values  (correlation range: P = -0.183 - -0.004, NP = -0.072 - 0.088; P-value range: P = 0.376 - 0.983 NP = 0.683 - 0.923). The other variables were similar to each other with high correlation values and low P-values (correlation range: P = 0.654 - 0.969, NP = 0.539 - 0.895; P-value range: P = 0.000 - 0.001 NP = 0.001 - 0.010). Parametric methods returned really high correlation values, lowest (most significant) P-values, and the correlation with zooplankton body mass ranked from highest to lowest is TIN, TN, TP, SRP, CHLA. Non-parametric methods had more moderate values but the p-values were still significant and the correlation with zooplankton body mass ranked from highest to lowest TN, TP, TIN, SRP, CHLA. The Pearson's method has evidence for false discovery rate but only in the CHLA P-Values (I actually see more evidence for false discovery rate in the Spearman method and I'm not sure if that was supposed to happen). False discovery rate is important because they will skew the significance because it increases the chance to reject the null-hypothesis with an increase in tests done. 


### Linear Regression

In the R code chunk below, do the following:
1) Conduct a linear regression analysis to test the relationship between total nitrogen (TN) and zooplankton biomass (ZP).
2) Examine the output of the regression analysis.
3) Produce a plot of this regression analysis including the following: categorically labeled points, the predicted regression line with 95% confidence intervals, and the appropriate axis labels.

```{r}


fitreg <- lm(ZP ~ TN, data = meso)
summary(fitreg)

plot.new()
plot(meso$TN, meso$ZP, ylim = c(0,10), xlim = c(500, 5000), xlab = expression(paste("Total Nitrogen (", mu, "g/L)")), ylab = "Zooplankton Biomass (mg/L)", las = 1)
text(meso$TN, meso$ZP, meso$NUTS, pos = 3, cex = 0.8)
newTN <- seq(min(meso$TN), max(meso$TN), 10)
regline <- predict(fitreg, newdata = data.frame(TN = newTN))
lines(newTN, regline)
conf95 <- predict(fitreg, newdata = data.frame(TN = newTN), interval = c("confidence"), level = 0.95, type = "response")
matlines(newTN, conf95[, c("lwr", "upr")], type ="l", lty = 2, lwd = 1, col = "black")
par(mfrow = c(2,2), mar = c(5.1,4.1,4.1,2.1))
plot(fitreg)
dev.off()
```

***Question 5***: Interpret the results from the regression model

> Answer 5: The P-value is very low so the results are statistically significant, but the adjusted R-squared value is ~0.55 so about half of the variation can be explained by the regression model so zooplankton body mass is related to total Nitrogen, but total Nitrogen wont be the most accurate in predicting zooplankton body mass. The residuals aren't randomly distributed around zero so TN doesn't fully explain ZP. In the QQ plot, the relationship is generally linear but there are outliers and most values fall below the line. The sqrt of the residuals are not linerally related to the fitted values. It doesn't look like values fall outside of the Cook's distance line but there are many values >|1|. In general, the values aren't normally distributed and aren't homoscedastic.


```{r}

```

### Analysis of Variance (ANOVA)

Using the R code chunk below, do the following:
1) Order the nutrient treatments from low to high (see handout).
2) Produce a barplot to visualize zooplankton biomass in each nutrient treatment.
3) Include error bars (+/- 1 sem) on your plot and label the axes appropriately.
4) Use a one-way analysis of variance (ANOVA) to test the null hypothesis that zooplankton biomass is affected by the nutrient treatment.


```{r}

NUTS <- factor(meso$NUTS, levels = c('L', 'M', 'H'))
zp.means <- tapply(meso$ZP, NUTS, mean)
sem <- function(x){sd(na.omit(x))/sqrt(length(na.omit(x)))}
zp.sem <- tapply(meso$ZP, NUTS, sem)
plot.new()
bp <- barplot(zp.means, ylim = c(0, round(max(meso$ZP), digits = 0)), pch = 15, cex = 1.25, las = 1, cex.lab = 1.4, cex.axis = 1.25, xlab = "nutrient supply", ylab = "zooplankton biomass (mg/L)", names.arg = c("low","medium","high"))
arrows(x0 = bp, y0 = zp.means, y1 = zp.means - zp.sem, angle = 90, length = 0.1, lwd = 1)
arrows(x0 = bp, y0 = zp.means, y1 = zp.means + zp.sem, angle = 90, length = 0.1, lwd = 1)
dev.off()
fitanova <- aov(ZP ~ NUTS, data = meso)
summary(fitanova)
TukeyHSD(fitanova)
plot.new()
par(mfrow = c(2,2), mar = c(5.1, 4.1, 4.1, 2.1))
plot(fitanova)
```

## SYNTHESIS: SITE-BY-SPECIES MATRIX

In the R code chunk below, load the zoops.txt data set in your **3.RStudio** data folder.
Create a site-by-species matrix (or dataframe) that does *not* include TANK or NUTS.
The remaining columns of data refer to the biomass (Âµg/L) of different zooplankton taxa: 
  
  + CAL = calanoid copepods
  
  + DIAP = *Diaphanasoma* sp. 
  
  + CYL = cyclopoid copepods
  
  + BOSM = *Bosmina* sp.
  
  + SIMO = *Simocephallus* sp.
  
  + CERI = *Ceriodaphnia* sp.
  
  + NAUP = naupuli (immature copepod)
  
  + DLUM = *Daphnia lumholtzi*
  
  + CHYD = *Chydorus* sp. 

***Question 6***: With the visualization and statistical tools that we learned about in the **3. RStudio** handout, use the site-by-species matrix to assess whether and how different zooplankton taxa were responsible for the total biomass (ZP) response to nutrient enrichment. 
Describe what you learned below in the "Answer" section and include appropriate code in the R chunk.

```{r}
zoops <- read.table("data/zoops.txt", sep = "\t", header = TRUE)
str(zoops)
nonut <- zoops[,3:11]
str(nonut)
bigZP <- c(meso.num[,6]*1000) #convert ZP into micrograms
nonutZP <- cbind(nonut,bigZP) #add converted ZP to zooplankton set
#I thought that the sum of the biomass for each species of zooplankton would sum up to ZP but it didn't (sometime ZP was way bigger or way smaller), I did notice that the biomass of Chydorus sp. was drastically more than the other species
avgnonut <- apply(nonut, 2, mean)
sdnonut <- apply(nonut,2,sd)
rank(avgnonut) #this returned the order of each species average biomass, from largest to smallest: CHYD, SIMO, CERI, CYCL, DIAP, CAL, BOSM, NAUPP, DLUM. 
range(avgnonut) #the averages had a range of 0.275 micrograms to 2906.638 micrograms
#because the range is so drastic, the species with the larger biomass will have more impact on the total ZP response to nutrient enrichment. The value for CHYD is over 6x larger than the next largest value and over 10000x larger than the smallest value. I think that the shear magnitude of CHYD compared to everything else would bury the signals of the other species. I got a bit confused as to what this question was asking and by the time I realized I didn't understand it, it was already late Thursday night so I apologize if I was way off base. 

```

## SUBMITTING YOUR WORKSHEET
Use Knitr to create a PDF of your completed **3.RStudio_Worksheet.Rmd** document, push the repo to GitHub, and create a pull request.
Please make sure your updated repo include both the PDF and RMarkdown files.

This assignment is due on **Wednesday, January 24^th^, 2021 at 12:00 PM (noon)**.

